# ðŸ”„ Proceso ETL

## VisiÃ³n General

El proceso ETL (Extract, Transform, Load) estÃ¡ dividido en tres scripts principales que deben ejecutarse en orden:

1. **etl_dimensions.py** - Carga de dimensiones
2. **etl_fact_equipos.py** - Carga de hechos de equipos
3. **etl_fact_servicios.py** - Carga de hechos de servicios

## Flujo Detallado

### 1. ETL de Dimensiones

**Script**: `etl/etl_dimensions.py`

**PropÃ³sito**: Poblar todas las tablas dimensionales del DW.

#### Fases

##### A. ExtracciÃ³n (Extract)
```python
# Lectura de archivos fuente
- Maestro Equipos.csv
- Pacientes.xlsx
- Aseguradora y Capita.xlsx
- Maestro Medicamentos.csv
- Maestro Insumos Medicos.csv
- Pedidos Solicitados.csv
```

**Formatos soportados**:
- CSV (delimitadores: `;`, `,`)
- Excel (.xlsx)

##### B. TransformaciÃ³n (Transform)
- NormalizaciÃ³n de nombres de columnas (strip)
- ConversiÃ³n de tipos de datos
- Limpieza de valores nulos
- GeneraciÃ³n de natural keys

##### C. Carga (Load)

1. **Carga a Staging**:
```python
df.to_sql('stg_tabla', engine, schema='stg', if_exists='replace')
```

2. **PoblaciÃ³n de Dimensiones**:

**dim_aseguradora**:
```python
- Verificar si cÃ³digo existe
- Si existe: UPDATE
- Si no existe: INSERT
```

**dim_paciente**:
```python
- Verificar si documento existe
- Si existe: UPDATE atributos
- Si no existe: INSERT nuevo
```

**dim_equipo (SCD Tipo 2)**:
```python
1. Comparar equipo_nk actual vs nuevo
2. Si cambiÃ³:
   - UPDATE registro anterior: es_actual=FALSE, vigente_hasta=HOY
   - INSERT nueva versiÃ³n: es_actual=TRUE, vigente_desde=HOY
3. Si no cambiÃ³: no hacer nada
```

**dim_pedido**:
```python
- UPSERT usando ON CONFLICT
- Actualiza si existe, inserta si no
```

**dim_medicamento**:
```python
- UPSERT por cÃ³digo
```

#### EjecuciÃ³n

```bash
cd etl
python etl_dimensions.py
```

**Salida esperada**:
```
ðŸ“‚ Leyendo archivos fuente...
âœ… Archivos leÃ­dos correctamente

ðŸ“¥ Cargando staging...
âœ… Staging cargado correctamente

ðŸ”„ Poblando dim_aseguradora...
âœ… dim_aseguradora: 45 registros nuevos

ðŸ”„ Poblando dim_paciente...
âœ… dim_paciente: 1523 registros nuevos

ðŸ”„ Poblando dim_equipo (SCD Tipo 2)...
âœ… dim_equipo: 87 registros procesados

...

âœ… ETL DE DIMENSIONES COMPLETADO
```

**Tiempo estimado**: 2-5 minutos (depende del volumen)

---

### 2. ETL de Hechos - Equipos

**Script**: `etl/etl_fact_equipos.py`

**PropÃ³sito**: Cargar transacciones de equipos mÃ©dicos.

#### Fases

##### A. PreparaciÃ³n
1. Leer staging: `stg_reporte_equipos`
2. Limpiar y normalizar datos
3. Convertir fechas al formato correcto

##### B. Auto-PoblaciÃ³n de Dimensiones Faltantes

**dim_fecha**:
```python
- Extraer fechas Ãºnicas de transacciones
- Insertar en dim_fecha si no existen
```

**dim_paciente**:
```python
- Identificar pacientes no existentes
- Insertar con datos bÃ¡sicos (documento, nombre)
```

**dim_aseguradora**:
```python
- Identificar cÃ³digos faltantes
- Insertar con nombre genÃ©rico
```

##### C. Joins con Dimensiones
```python
# Normalizar keys para join
df['codigo_tipo_equipo_norm'] = df['CÃ³digo Interno'].str.upper()
df['documento_norm'] = df['Documento Paciente'].str.upper()

# Merge con dimensiones
df.merge(dim_equipo, ...)
df.merge(dim_paciente, ...)
df.merge(dim_aseguradora, ...)
df.merge(dim_fecha, ...)
```

##### D. ValidaciÃ³n
```python
# Verificar registros sin FK
sin_equipo = df[df['equipo_id'].isna()]
sin_paciente = df[df['paciente_id'].isna()]
sin_aseguradora = df[df['aseguradora_id'].isna()]
sin_fecha = df[df['fecha_id'].isna()]
```

##### E. Carga
```python
# Generar PK compuesta
solicitud_equipos_id = f"{Codigo}-{NumeroSerie}-{Fecha}"

# Insertar o actualizar
FOR cada registro:
    IF existe:
        UPDATE
    ELSE:
        INSERT
```

#### EjecuciÃ³n

```bash
cd etl
python etl_fact_equipos.py
```

**Salida esperada**:
```
============================================================
CARGANDO HECHO_EQUIPOS
============================================================
Registros en staging: 4532

Poblando dim_fecha...
dim_fecha actualizada: 234 fechas nuevas insertadas âœ…

Dimensiones cargadas:
  - dim_equipo: 87 registros
  - dim_paciente: 1523 registros
  - dim_aseguradora: 45 registros
  - dim_fecha: 876 registros

ðŸ“Š Registros vÃ¡lidos: 4498/4532

âœ… HECHO_EQUIPOS CARGADO:
   - Insertados: 4321
   - Actualizados: 177
   - Errores: 34

âœ… PROCESO COMPLETADO
```

**Tiempo estimado**: 5-15 minutos (depende del volumen)

---

### 3. ETL de Hechos - Servicios

**Script**: `etl/etl_fact_servicios.py`

**PropÃ³sito**: Cargar solicitudes de servicios e insumos.

#### CaracterÃ­sticas Especiales

##### UPSERT Masivo
Este script utiliza una tÃ©cnica optimizada para grandes volÃºmenes:

```python
1. Crear tabla temporal
2. Cargar datos a tabla temporal (bulk insert)
3. UPSERT desde tabla temporal a tabla final
4. Eliminar tabla temporal
```

**Ventajas**:
- 10-50x mÃ¡s rÃ¡pido que inserts individuales
- Manejo transaccional
- Menos locks en la tabla final

##### Manejo de Duplicados
```python
# Eliminar duplicados en memoria antes de cargar
df.drop_duplicates(subset=['solicitud_servicio_id'], keep='last')
```

#### Fases

##### A. Lectura de CSV
```python
df = pd.read_csv(
    'Insumos Solicitados HistÃ³rico Actualizado.csv',
    sep=None,  # Auto-detectar separador
    encoding='utf-8-sig'
)
```

##### B. TransformaciÃ³n
```python
# Normalizar columnas
- Servicio
- Numero de pedido
- Identificacion Paciente
- Fecha envio a logistica
- Aseguradora
- Estado del pedido

# Asignar cantidad default
df['cantidad'] = 1
```

##### C. PoblaciÃ³n de Dimensiones Auxiliares
```python
poblar_dim_fecha()
poblar_dim_pedido()  # Auto-crear pedidos faltantes
```

##### D. Carga Optimizada
```sql
-- Crear tabla temporal
CREATE TEMP TABLE temp_servicios (...)

-- Cargar con pandas (rÃ¡pido)
df.to_sql('temp_servicios', ...)

-- UPSERT masivo
INSERT INTO hecho_solicitud_servicios
SELECT * FROM temp_servicios
ON CONFLICT (solicitud_servicio_id) 
DO UPDATE SET ...

-- Limpiar
DROP TABLE temp_servicios
```

#### EjecuciÃ³n

```bash
cd etl
python etl_fact_servicios.py
```

**Salida esperada**:
```
============================================================
CARGANDO HECHO_SOLICITUD_SERVICIOS
============================================================

[1/6] Leyendo archivo CSV...
âœ“ Archivo cargado: 50450 registros

[2/6] Extrayendo de staging...
[3/6] Limpiando datos...

[4/6] Poblando dimensiones...
dim_fecha actualizada: 187 fechas nuevas insertadas âœ…
dim_pedido actualizada: 4321 pedidos nuevos insertados âœ…

[5/6] Haciendo joins con dimensiones...
âœ“ Dimensiones cargadas:
  - dim_pedido: 4321
  - dim_paciente: 1523
  - dim_aseguradora: 45
  - dim_fecha: 1063

ðŸ“Š Registros vÃ¡lidos: 50123/50450

[6/6] Cargando datos (usando UPSERT masivo)...
  â†’ Registros antes de eliminar duplicados: 50123
  â†’ Registros despuÃ©s de eliminar duplicados: 49876
  â†’ Creando tabla temporal...
  â†’ Insertando 49876 registros a tabla temporal...
  â†’ Ejecutando UPSERT masivo...

âœ… HECHO_SOLICITUD_SERVICIOS CARGADO:
   - Total procesado: 49876 registros
   - OperaciÃ³n: UPSERT masivo exitoso

â±ï¸  Tiempo total: 23.45 segundos

âœ… PROCESO COMPLETADO
```

**Tiempo estimado**: 20-60 segundos (con UPSERT masivo)

---

## Orden de EjecuciÃ³n

### Carga Inicial (Primera vez)

```bash
# 1. Crear estructura de BD
psql -U postgres -d dw_HHCC -f sql/01_create_schemas.sql
psql -U postgres -d dw_HHCC -f sql/02_create_staging_tables.sql
psql -U postgres -d dw_HHCC -f sql/03_create_dimension_tables.sql
psql -U postgres -d dw_HHCC -f sql/04_create_fact_tables.sql

# 2. Ejecutar ETL
python etl/etl_dimensions.py
python etl/etl_fact_equipos.py
python etl/etl_fact_servicios.py

# 3. Validar
psql -U postgres -d dw_HHCC -f sql/05_queries_validation.sql
```

### Carga Incremental (ActualizaciÃ³n)

```bash
# Solo ejecutar ETLs con archivos actualizados
python etl/etl_dimensions.py       # Si hay cambios en maestros
python etl/etl_fact_equipos.py     # Si hay nuevos equipos
python etl/etl_fact_servicios.py   # Si hay nuevos servicios
```

---

## Manejo de Errores

### Errores Comunes

#### 1. Archivo No Encontrado
```
âŒ Error: No se encontrÃ³ el archivo /path/to/file.csv
```
**SoluciÃ³n**: Verificar rutas en `DATA_DIR`

#### 2. Error de ConexiÃ³n
```
âŒ Error de conexiÃ³n: FATAL:  password authentication failed
```
**SoluciÃ³n**: Verificar credenciales en `config/database_config.py`

#### 3. Registros Sin FK
```
âš ï¸  234 registros sin equipo_id
```
**SoluciÃ³n**: 
- Revisar cÃ³digos en staging vs dimensiones
- Ejecutar primero `etl_dimensions.py`
- Verificar normalizaciÃ³n de keys

#### 4. Duplicados en PK
```
ERROR: duplicate key value violates unique constraint
```
**SoluciÃ³n**: 
- Ya manejado automÃ¡ticamente con UPSERT
- Si persiste: eliminar duplicados manualmente

### Logs y Debugging

#### Activar modo verbose
```python
engine = create_engine(CONN_STR, echo=True)  # Muestra SQL
```

#### Ver registros problemÃ¡ticos
```sql
-- En hecho_equipos
SELECT * FROM dw_HHCC.hecho_equipos 
WHERE equipo_id IS NULL OR paciente_id IS NULL;

-- En staging
SELECT * FROM stg.stg_reporte_equipos
WHERE "CÃ³digo Interno" NOT IN (
    SELECT equipo_nk FROM dw_HHCC.dim_equipo
);
```

---

## Optimizaciones

### Mejoras de Performance

1. **Ãndices**: Creados automÃ¡ticamente en FKs
2. **Bulk Inserts**: Uso de `method='multi'`
3. **Tablas Temporales**: Para UPSERT masivo
4. **Transacciones**: Un commit al final

### ConfiguraciÃ³n PostgreSQL

```ini
# postgresql.conf
shared_buffers = 256MB
work_mem = 16MB
maintenance_work_mem = 128MB
effective_cache_size = 1GB
```

### ParalelizaciÃ³n (Futuro)

```python
# Cargar hechos en paralelo
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=2) as executor:
    f1 = executor.submit(cargar_hecho_equipos)
    f2 = executor.submit(cargar_hecho_servicios)
```

---

## Monitoreo

### MÃ©tricas a Revisar

```sql
-- Tiempo de Ãºltima carga
SELECT 
    'hecho_equipos' as tabla,
    MAX(fecha_carga) as ultima_carga,
    COUNT(*) as total_registros
FROM dw_HHCC.hecho_equipos
UNION ALL
SELECT 
    'hecho_solicitud_servicios',
    MAX(fecha_carga),
    COUNT(*)
FROM dw_HHCC.hecho_solicitud_servicios;
```

### Alertas Recomendadas

- Carga fallida (exit code != 0)
- MÃ¡s de 5% de registros rechazados
- Tiempo de ejecuciÃ³n > 2x promedio
- Espacio en disco < 20%

---

## Checklist de ValidaciÃ³n

### Post-ETL

- [ ] Conteo de registros staging vs DW
- [ ] No hay FKs nulas en hechos
- [ ] Fechas vÃ¡lidas en dim_fecha
- [ ] SCD2 funcionando (mÃºltiples versiones)
- [ ] MÃ©tricas suman correctamente
- [ ] Consultas de validaciÃ³n sin errores

```sql
-- Ejecutar
\i sql/05_queries_validation.sql
```
